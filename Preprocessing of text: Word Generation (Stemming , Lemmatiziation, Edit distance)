Software ----> collab


#Edit Distance
def editDistDP(str1, str2, m, n):
  # Create a table to store results of subproblems
  dp = [[0 for x in range(n + 1)] for x in range(m + 1)]
  # Fill d[][] in bottom up manner
  for i in range(m + 1):
    for j in range(n + 1):
      # If first string is empty, only option is to
      # insert all characters of second string
      if i == 0:
        dp[i][j] = j # Min. operations = j
      # If second string is empty, only option is to
      # remove all characters of second string
      elif j == 0:
        dp[i][j] = i # Min. operations = i
      # If last characters are same, ignore last char
      # and recur for remaining string
      elif str1[i-1] == str2[j-1]:
        dp[i][j] = dp[i-1][j-1]
      # If last character are different, consider all
      # possibilities and find minimum
      else:
        dp[i][j] = 1 + min(dp[i][j-1],   # Insert
                dp[i-1][j],  # Remove
                dp[i-1][j-1]) # Replace
  return dp[m][n]

# Driver code
str2 = &quot;dof&quot;
str1 = &quot;dog&quot;


import nltk
nltk.download(&#39;punkt&#39;)


from nltk.tokenize import sent_tokenize
text=&quot;&quot;&quot;Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.
The sky is pinkish-blue. You shouldn&#39;t eat cardboard&quot;&quot;&quot;
tokenized_text=sent_tokenize(text)
print(tokenized_text)


#word token
from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
print(tokenized_word)


from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
ps = PorterStemmer()
stemmed_words=[]
for w in tokenized_word:
    stemmed_words.append(ps.stem(w))
print(&quot;Filtered Sentence:&quot;,tokenized_word)
print(&quot;Stemmed Sentence:&quot;,stemmed_words)


nltk.download(&#39;wordnet&#39;)
nltk.download(&#39;omw-1.4&#39;)


#Lexicon Normalization
#performing stemming and Lemmatization
from nltk.stem.wordnet import WordNetLemmatizer
lem = WordNetLemmatizer()
from nltk.stem.porter import PorterStemmer
stem = PorterStemmer()
word = &quot;flagged&quot;
print(&quot;Lemmatized Word:&quot;,lem.lemmatize(word,&quot;v&quot;))
print(&quot;Stemmed Word:&quot;,stem.stem(word))


import nltk
from nltk.stem.porter import PorterStemmer
porter_stemmer  = PorterStemmer()
text = &quot;studies studying cries cry&quot;
tokenization = nltk.word_tokenize(text)
for w in tokenization:
  print(&quot;Stemming for {} is {}&quot;.format(w,porter_stemmer.stem(w)))


import nltk
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
text = &quot;studies studying cries cry&quot;
tokenization = nltk.word_tokenize(text)
for w in tokenization:
  print(&quot;Lemma for {} is {}&quot;.format(w, wordnet_lemmatizer.lemmatize(w)))


# Import the toolkit and the full Porter Stemmer library
import nltk
from nltk.stem.porter import *
p_stemmer = PorterStemmer()
words = [&#39;run&#39;,&#39;runner&#39;,&#39;running&#39;,&#39;ran&#39;,&#39;runs&#39;,&#39;easily&#39;,&#39;fairly&#39;]
for word in words:
    print(word+&#39; --&gt; &#39;+p_stemmer.stem(word))


# Perform standard imports:
import spacy
nlp = spacy.load(&#39;en_core_web_sm&#39;)
def show_lemmas(text):
for token in text:
        print(f&#39;{token.text:{12}} {token.pos_:{6}} {token.lemma:&lt;{22}} {token.lemma_}&#39;)


